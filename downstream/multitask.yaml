runner:
  total_steps: 200000
  gradient_clipping: 1
  gradient_accumulate_steps: 1
  save_step: 5000
  max_keep: 1

optimizer:
  name: TorchOptim
  torch_optim_name: AdamW
  lr: 1.0e-5

# comment the whole scheduler config block
# to disable learning rate scheduling
#
scheduler:
  name: linear_schedule_with_warmup
  num_warmup_steps: 5000

# comment the whole specaug config block
# to disable specaug on representation
#
specaug:
  apply_time_warp: true
  apply_time_mask: true
  apply_freq_mask: true
  time_warp_window: 5
  time_mask_width_range: [0, 40]
  freq_mask_width_range: [0, 50]
  num_freq_mask: 4
  num_time_mask: 2

pr:
  folder: ctc
  looprc:
    log_step: 100
    eval_step: 5000
    eval_dataloaders:  # The first loader will be used to save checkpoint
      - dev
      - test

  corpus:                                 
    name: 'libriphone'                   # Specify corpus
    path: '/LibriSpeech_grt'          # Path to raw LibriSpeech dataset
    lexicon:
      # - downstream/ctc/lexicon/librispeech-lexicon.txt
      - downstream/ctc/lexicon/librispeech-lexicon-200k-g2p.txt
      - downstream/ctc/lexicon/librispeech-lexicon-allothers-g2p.txt

    train: ['train-clean-100']                # Name of data splits to be used as training set
    dev: ['dev-clean']                    # Name of data splits to be used as validation set
    test: ['test-clean']

    bucketing: True                       # Enable/Disable bucketing 
    batch_size: 1
    num_workers: 24
  
  text:
    mode: 'word'                       # 'character'/'word'/'subword'
    vocab_file: 'downstream/ctc/vocab/phoneme.txt'
  
  model:
    project_dim: 256
    zero_infinity: True

    select: FrameLevel
    Wav2Letter:
      total_rate: 320
    RNNs:
      total_rate: 320
      module: 'LSTM'                        # 'LSTM'/'GRU'
      bidirection: True
      dim: [1024, 1024, 1024]
      dropout: [0.2, 0.2, 0.2]
      layer_norm: [True, True, True]
      proj: [True, True, True]              # Linear projection + Tanh after each rnn layer
      sample_rate: [1, 1, 1]
      sample_style: 'concat'                  # 'drop'/'concat'

  save_best_on:
    - dev

  metric_higher_better: False
  metric:  # The first metric will be used to save checkpoint
    - wer

ks:
  folder: speech_commands
  looprc:
    log_step: 100
    eval_step: 5000
    eval_dataloaders:
      - dev
      - test
  
  datarc:
    speech_commands_root: /SpeechNet/datasets/speech_commands_v0.01
    speech_commands_test_root: /SpeechNet/datasets/speech_commands_test_set_v0.01
    num_workers: 8
    batch_size: 1

  modelrc:
    projector_dim: 256
    select: UtteranceLevel
    UtteranceLevel:
      pooling: MeanPooling

ic:
  folder: fluent_commands
  looprc:
    log_step: 100
    eval_step: 5000
    eval_dataloaders:
      - dev
      - test

  datarc:
    file_path: /SpeechNet/datasets/fluent_speech_commands_dataset
    num_workers: 12
    train_batch_size: 1
    eval_batch_size: 8

  modelrc:
    projector_dim: 256
    select: UtteranceLevel
    UtteranceLevel:
      pooling: MeanPooling

sid:
  folder: voxceleb1
  looprc:
    log_step: 100
    eval_step: 5000
    eval_dataloaders:
      - dev
      - test

  datarc:
    file_path: /VoxCeleb1
    meta_data: ./downstream/voxceleb1/veri_test_class.txt
    num_workers: 12
    train_batch_size: 1
    eval_batch_size: 4
    max_timestep: 128000

  modelrc:
    projector_dim: 256
    select: UtteranceLevel
    UtteranceLevel:
      pooling: MeanPooling

er:
  folder: emotion
  looprc:
    log_step: 100
    eval_step: 5000
    eval_dataloaders:
      - dev
      - test

  datarc:
    root: /SpeechNet/datasets/IEMOCAP
    pre_load: True
    train_batch_size: 1
    eval_batch_size: 4
    num_workers: 12
    valid_ratio: 0.2
    test_fold: fold1

  modelrc:
    projector_dim: 256
    select: UtteranceLevel

    UtteranceLevel:
      pooling: MeanPooling

    DeepModel:
      model_type: CNNSelfAttention
      input_dim: 256
      hidden_dim: 80
      kernel_size: 5
      padding: 2
      pooling: 5
      dropout: 0.4

asr:
  folder: asr
  looprc:
    log_step: 100
    eval_step: 5000
    eval_dataloaders:
      - dev-clean
      - test-clean
      - dev-other
      - test-other

  datarc:
    train: ['train-clean-100']
    dev-clean: ['dev-clean']
    dev-other: ['dev-other']
    test-clean: ['test-clean']
    test-other: ['test-other']
    num_workers: 12
    train_batch_size: 1
    eval_batch_size: 4
    libri_root: '/LibriSpeech_grt'
    bucket_file: 'data/LibriSpeech/len_for_bucket'

    zero_infinity: True

    decoder_args:
      # See https://github.com/facebookresearch/flashlight/blob/master/flashlight/lib/text/decoder/LexiconDecoder.h#L20
      # for what the options mean. Python binding exposes the same options from C++.
      # KenLM is a fast LM query implementation, and it can be powered by:
      # 1. official LibriSpeech 4-gram LM: the 4-gram.arpa file on http://www.openslr.org/11
      # 2. fairseq style, letter-based lexicon: https://dl.fbaipublicfiles.com/fairseq/wav2vec/librispeech_lexicon.lst
      decoder_type: 'None'
      nbest: 1
      criterion: "ctc"
      beam: 5
      beam_threshold: 25
      kenlm_model: '/path/to/KenLM'
      lexicon: '/path/to/4-gram.arpa'
      lm_weight: 2
      word_score: -1
      unk_weight: -math.inf
      sil_weight: 0

  modelrc:
    project_dim: 1024
    select: RNNs
    Wav2Letter:
      total_rate: 320
    RNNs:
      total_rate: -1
      module: 'LSTM'                        # 'LSTM'/'GRU'
      bidirection: True
      dim: [1024, 1024]
      dropout: [0.2, 0.2]
      layer_norm: [False, False]
      proj: [False, False]              # Linear projection + Tanh after each rnn layer
      sample_rate: [1, 1]
      sample_style: 'concat'                  # 'drop'/'concat'

sf:
  folder: ctc
  looprc:
    log_step: 100
    eval_step: 5000
    eval_dataloaders:
      - valid
      - test

  corpus:                                 
    name: 'snips'                   # Specify corpus
    path: '/SpeechNet/datasets/SNIPS'          # Path to raw LibriSpeech dataset

    train: ['train']                # Name of data splits to be used as training set
    valid: ['valid']                    # Name of data splits to be used as validation set
    test: ['test']

    bucketing: True                       # Enable/Disable bucketing 
    batch_size: 1
    train_speakers: 
    # - Aditi
    # - Amy
    # - Brian
    # - Emma
    # - Geraint
    - Ivy
    - Joanna
    - Joey
    - Justin
    - Kendra
    - Kimberly
    - Matthew
    # - Nicole
    # - Raveena
    # - Russell
    - Salli
    valid_speakers:
    - Aditi
    - Amy
    # - Brian
    # - Emma
    - Geraint
    # - Ivy
    # - Joanna
    # - Joey
    # - Justin
    # - Kendra
    # - Kimberly
    # - Matthew
    - Nicole
    # - Raveena
    # - Russell
    # - Salli
    test_speakers:
    # - Aditi
    # - Amy
    - Brian
    - Emma
    # - Geraint
    # - Ivy
    # - Joanna
    # - Joey
    # - Justin
    # - Kendra
    # - Kimberly
    # - Matthew
    # - Nicole
    - Raveena
    - Russell
    # - Salli
    num_workers: 12
  
  text:
    mode: 'character-slot'                       # 'character'/'word'/'subword'
    vocab_file: 'downstream/ctc/vocab/character.txt'
    slots_file: '/SpeechNet/datasets/SNIPS/slots.txt'
  
  model:
    project_dim: 1024
    zero_infinity: True

    select: RNNs
    Wav2Letter:
      total_rate: 320
    RNNs:
      total_rate: -1
      module: 'LSTM'                        # 'LSTM'/'GRU'
      bidirection: True
      dim: [1024, 1024]
      dropout: [0.2, 0.2]
      layer_norm: [False, False]
      proj: [False, False]              # Linear projection + Tanh after each rnn layer
      sample_rate: [1, 1]
      sample_style: 'concat'                  # 'drop'/'concat'

  save_best_on:
    - valid

  metric_higher_better: True
  metric:  # The first metric will be used to save checkpoint
    - slot_type_f1
    - slot_value_cer
    - slot_value_wer
    - slot_edit_f1_full
    - slot_edit_f1_part
    - wer
    - cer

sv:
  folder: sv_voxceleb1
  looprc:
    log_step: 100
    eval_step: 1000000
    eval_dataloaders:
      - test

  datarc:
    # In first time, we filter out utterances which shorter than two sec. (change config just influenced by cache files)
    # If you change min_sec, you need to delete cache file you create and rerun the code, so that the effect will apply.
    vad_config:
      min_sec: 32000
    
    file_path: /VoxCeleb1
    train_meta_data: ./downstream/sv_voxceleb1/dev_meta_data/dev_speaker_ids.txt
    dev_meta_data: ./downstream/sv_voxceleb1/dev_meta_data/dev_meta_data.txt
    test_meta_data: ./downstream/sv_voxceleb1/voxceleb1_test_v2.txt

    max_timestep: 128000
    train_batch_size: 1
    eval_batch_size: 4
    num_workers: 8 

  modelrc:
    module:
      XVector  # support to [ XVector, Identity ]
    input_dim: 512
    agg_module: SP # support for ASP / SP / AP / MP 
                   # (Attentive Statistic Pooling / Statistic Pooling / Attentive Pooling / Mean Pooling)
    utter_module:
      UtteranceExtractor # support to [UtteranceExtractor, UtteranceIdentity]
    
    module_config:
      # You can comment it if you do not use this. To demo the usage, we will show all case.
      XVector:
        agg_dim: 1500
        dropout_p: 0.0
        batch_norm: False
      
      Identity:
        no_args: True
        # do nothing    
    
    ObjectiveLoss: AMSoftmaxLoss # You can specify config to AMSoftmaxLoss or SoftmaxLoss
    
    LossConfig:
      # You can comment it if you do not use this. To demo the usage of SoftmaxLoss, we will show all case.
      SoftmaxLoss: 
        no_args: True
    
      # You can comment it if you do not use this. To demo the usage of AMSoftmaxLoss, we will show all case.
      AMSoftmaxLoss:
        s: 30.0
        m: 0.4

sd:
  folder: diarization
  looprc:
    log_step: 100
    eval_step: 5000
    eval_dataloaders:
      - dev
      - test

  datarc:
    chunk_size: 2000
    frame_shift: 160 # this should be aligned with upstrema model
    subsampling: 1
    label_delay: 0
    num_speakers: 2
    rate: 16000

  loaderrc:
    num_workers: 8
    train_batchsize: 1
    eval_batchsize: 4
    train_dir: ./downstream/diarization/data/train
    dev_dir: ./downstream/diarization/data/dev
    test_dir: ./downstream/diarization/data/test
  
  scorerc:
    save_predictions: True

  modelrc: 
    rnn_layers: 1
    hidden_size: 512
