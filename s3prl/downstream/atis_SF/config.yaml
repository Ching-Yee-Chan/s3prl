runner:
  total_steps: 20000
  gradient_clipping: 1
  gradient_accumulate_steps: 96

  log_step: 5
  eval_step: 30
  save_step: 300
  max_keep: 1
  eval_dataloaders:
    - dev
    - test

optimizer:
  name: AdamW
  lr: 2.0e-4

# comment the whole scheduler config block
# to disable learning rate scheduling
scheduler:
  name: linear_schedule_with_warmup
  num_warmup_steps: 100

downstream_expert:
  datarc:
    file_path: /home/daniel094144/data/atis
    num_workers: 0
    train_batch_size: 2
    eval_batch_size: 1
    is_BI: False
    is_BIO: True
    unit_path: /home/daniel094144/data/atis/atis_code_128_IN/code
    # unit_path: /home/daniel094144/data/atis/atis_code_128_IN/char_code
    unit_size: 128
    # unit_tokenizer: /home/daniel094144/data/atis/atis_code_128_IN/tokenizer_1000.json
  
  augmentation: 
    TimeStretch: 
      min_rate: 0.9
      max_rate: 1.1
      p: 0.5
    # PitchShift: 
    #   min_semitones: -4
    #   max_semitones: 4
    #   p: 0.5

  modelrc:
    is_transformer: True
    vocab_size: 600
    input_dim: 512
    ctc_weight: 0
    num_encoder_layers: 3
    num_decoder_layers: 6
    emb_size: 512
    nhead: 4
    dim_feedforward: 2048
    is_dual_decoder: True
    unit_decode_weight: 0.3
    label_smoothing: 0.05